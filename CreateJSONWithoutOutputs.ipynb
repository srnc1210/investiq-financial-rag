{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_kM5qI1Q-PG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "x3T41D85R1HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#my unique google drive path:\n",
        "project_path = \"/content/drive/MyDrive/Uni/DS 5983/Final Project/Data/\" # Nicholai path\n",
        "# project_path = \"/content/drive/MyDrive/DS5983-FinalProjcet/\" # Tyree's path\n",
        "os.chdir(project_path)"
      ],
      "metadata": {
        "id": "vTdgtnDgVUgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Cxg6TXHhSjzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "8Ixp2MiVVnZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL = \"sujet-ai/Marsilia-Embeddings-EN-Large\" # this was the first model\n",
        "#EMBEDDING_MODEL = \"ProsusAI/finbert\" # trained on sentiment classification, so it needed to append mean-pooling layer for sentence transformer\n",
        "#EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # General purpose, fast\n",
        "#EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"  # Higher quality\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "model = SentenceTransformer(EMBEDDING_MODEL, device=device)"
      ],
      "metadata": {
        "id": "7HHKNCsdSdbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# switch function for chunking type\n",
        "def choose_chunking(doc, type=\"sentence\"):\n",
        "  if type == \"sentence\":\n",
        "    return sentence_chunks(doc)\n",
        "  elif type == \"fixed\":\n",
        "    return read_file_in_chunks_fixed(doc)\n",
        "  elif type == \"overlap\":\n",
        "    return read_file_in_chunks_overlap(doc)\n",
        "\n",
        "# sentence chunking\n",
        "def sentence_chunks(doc):\n",
        "  with open(doc, 'r') as file:\n",
        "    chunks = [line for line in file.readlines() if line.strip()]\n",
        "  return chunks\n",
        "\n",
        "\n",
        "# Fixed word chunking\n",
        "def read_file_in_chunks_fixed(doc, target_words=200):\n",
        "    with open(doc, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # array of single words\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    # 0 to end of words, step by 'target_words'\n",
        "    for i in range(0, len(words), target_words):\n",
        "        chunk = ' '.join(words[i:i + target_words])\n",
        "        if chunk.strip():  # Only add non-empty chunks\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def read_file_in_chunks_overlap(doc, target_words=100, overlap_words=10):\n",
        "    with open(doc, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    #step of target_words not constant like above: will likely not have full 200 chunk if find period\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        # Get chunk of full target-size\n",
        "        chunk_words = words[i:i + target_words]\n",
        "        chunk = ' '.join(chunk_words)\n",
        "\n",
        "        # Try to end at a sentence boundary if possible\n",
        "        if i + target_words < len(words):\n",
        "            # Look for sentence end in last 20% of chunk\n",
        "            last_20_percent_words = int(target_words * 0.2)\n",
        "\n",
        "            last_portion = ' '.join(chunk_words[-last_20_percent_words:])\n",
        "            if '.' in last_portion:\n",
        "                last_period = chunk.rfind('.')\n",
        "                chunk = chunk[:last_period + 1]\n",
        "                actual_words = len(chunk.split())\n",
        "                i += actual_words - overlap_words\n",
        "            else:\n",
        "                i += target_words - overlap_words\n",
        "        else:\n",
        "            i += target_words\n",
        "\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk.strip())\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "NohT6Na4ygTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#placeHold = {\"chunk_id\":-1, \"chunk_text\":\"temp\",\"embed\":[0]}\n",
        "CHUNK_OPTION = \"overlap\" # 'sentence', \"fixed\" or \"overlap\"\n",
        "data_dirs = [\"FOMC Data\", \"Company Data/Earnings Calls\", \"Company Data/Annual Reports\"]\n",
        "\n",
        "fomc_docs = [\"FOMC_min_JAN25.txt\", \"FOMC_min_MARCH25.txt\", \"FOMC_min_MAY25.txt\", \"FOMC_min_JUNE25.txt\"]\n",
        "earn_docs = [\"Alphabet_GOOGL_Q2_2025_Earnings_Call_Transcript.txt\", \"Amazon_Q1_MAY2025_Earnings_Call.txt\",\n",
        "             \"APPL_EarningsCall_Q1_JAN25.txt\", \"META-Q1-2025-Earnings-Call-Transcript-1.txt\",\n",
        "             \"MSFT_EarningsCall_Q3_APRIL25.txt\"]\n",
        "annual_docs = [\"microsoft_2024_structured.txt\", \"meta_2024_structured.txt\", \"google_2024_structured.txt\",\n",
        "               \"amazon_2024_structured.txt\",\"apple_2024_structured.txt\"] #\"AAPL_10-k_24.txt\" IT is a google doc still\n",
        "jsonDict = {}\n",
        "for dir in data_dirs:\n",
        "  os.chdir(dir)\n",
        "\n",
        "  if dir == \"FOMC Data\":\n",
        "    docs = fomc_docs\n",
        "  elif dir == \"Company Data/Earnings Calls\":\n",
        "    docs = earn_docs\n",
        "  elif dir == \"Company Data/Annual Reports\":\n",
        "    docs = annual_docs\n",
        "  else:\n",
        "    print(\"ERROR WITH DIRS PASSED\")\n",
        "    break\n",
        "\n",
        "  for doc in docs:\n",
        "    # simple sentence seperation\n",
        "    # with open(doc, 'r') as file:\n",
        "    #   dataset = [line for line in file.readlines() if line.strip()]\n",
        "    dataset = choose_chunking(doc, type=CHUNK_OPTION)\n",
        "\n",
        "    # embed each line (each line gets seperate embedding)\n",
        "    embeddings = model.encode(dataset, batch_size=32, normalize_embeddings=True)\n",
        "\n",
        "    jsonDict[doc] = [] # key is doc name, value is list of chunks\n",
        "\n",
        "    for i, (text, embedding) in enumerate(zip(dataset, embeddings)):\n",
        "      jsonDict[doc].append({\n",
        "          \"chunk_id\": i,\n",
        "          \"chunk_text\": text,\n",
        "          \"embed\": embedding.tolist()\n",
        "      })\n",
        "\n",
        "    print(\"Document:\", doc)\n",
        "    print(f'Loaded {len(jsonDict[doc])} entries')\n",
        "  # reset directory back to base\n",
        "  os.chdir(project_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "wpX2JC5LSHEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Vector Database\n",
        "with open(\"VectorDB_Mar_100_Window.json\", \"w\") as file:\n",
        "    json.dump(jsonDict, file)"
      ],
      "metadata": {
        "id": "OMOkHHuZuSnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample of how to access\n",
        "jsonDict['FOMC_min_JAN25.txt'][32]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7raYR0LwtVy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examining Average Length of Chunk**"
      ],
      "metadata": {
        "id": "gCojgB3Uusam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Uni/DS 5983/Final Project/Data/VectorDB.json' # nicholai\n",
        "# path = '/content/drive/MyDrive/Final Project/Data/VectorDB.json' # shreya\n",
        "# path = '/content/drive/MyDrive/DS5983-FinalProjcet/VectorDB.json' # tyree\n",
        "\n",
        "with open(path, \"r\") as file:\n",
        "    Vector_db = json.load(file)"
      ],
      "metadata": {
        "id": "UfOkeIS0ubJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_chunks = []\n",
        "\n",
        "# Or don't load (analyze immediatly after):\n",
        "Vector_db = jsonDict\n",
        "for doc_name, chunks in Vector_db.items():\n",
        "    for chunk in chunks:\n",
        "        flat_chunks.append(chunk['chunk_text'])\n"
      ],
      "metadata": {
        "id": "IAa2A2E9uq9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = [len(chunk.split()) for chunk in flat_chunks]\n",
        "\n",
        "# Calculate average\n",
        "average_words = sum(word_counts) / len(word_counts) if word_counts else 0\n",
        "\n",
        "print(f\"Average words per chunk: {average_words:.2f}\")\n",
        "print(f\"Total chunks: {len(flat_chunks)}\")\n",
        "print(f\"Min words: {min(word_counts) if word_counts else 0}\")\n",
        "print(f\"Max words: {max(word_counts) if word_counts else 0}\")"
      ],
      "metadata": {
        "id": "W3Xapfdnuq67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_index = word_counts.index(max(word_counts))\n",
        "\n",
        "# Get the chunk with max words\n",
        "max_chunk = flat_chunks[max_index]\n",
        "max_chunk"
      ],
      "metadata": {
        "id": "GPlQB-6Muq39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notice any overlap (default is 10 words) (for overlap chunking style)\n",
        "for i in range(5):\n",
        "  print(flat_chunks[i], \"\\n\" )"
      ],
      "metadata": {
        "id": "bvsqLZDJ6rv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4X7Q9FhwccNr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}