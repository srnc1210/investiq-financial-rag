{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["kYDtitxNi3Nu","kYXzZtvejDDb"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### Installations of libraries and Setting environment variables"],"metadata":{"id":"P5LKZsjVCzpi"}},{"cell_type":"code","source":["from google.colab import drive\n","import torch\n","import json\n","drive.mount('/content/drive')"],"metadata":{"id":"lVpOZeoWev2n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755313706717,"user_tz":240,"elapsed":18134,"user":{"displayName":"Nicholai Platonoff","userId":"15925676260455771154"}},"outputId":"25e1016e-2e04-49a7-905b-2cb4417a06f3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# faiss is for dot product\n","!pip install sentence-transformers faiss-cpu rouge_score nltk -q"],"metadata":{"id":"DH2boIWMDbBu","executionInfo":{"status":"ok","timestamp":1755313831573,"user_tz":240,"elapsed":124852,"user":{"displayName":"Nicholai Platonoff","userId":"15925676260455771154"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1685244-0242-46f5-b75b-507c1f21dcb5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {DEVICE}\")"],"metadata":{"id":"rGXkv1FwE7Im","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755313831613,"user_tz":240,"elapsed":28,"user":{"displayName":"Nicholai Platonoff","userId":"15925676260455771154"}},"outputId":"7d7dd618-9c7e-431c-c52c-b2ca7b8cc03e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"code","source":["!hf auth login"],"metadata":{"id":"V-YgX3fN-aDu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Variables"],"metadata":{"id":"GTqyDR6UC_3j"}},{"cell_type":"code","source":["file_path = '/content/drive/MyDrive/Uni/DS 5983/Final Project/Data' # nicholai\n","# file_path = '/content/drive/MyDrive/Final Project/Data' # shreya\n","# file_path = '/content/drive/MyDrive/DS5983-FinalProjcet' # tyree"],"metadata":{"id":"FyE9sdEUDB6r","executionInfo":{"status":"ok","timestamp":1755313831637,"user_tz":240,"elapsed":4,"user":{"displayName":"Nicholai Platonoff","userId":"15925676260455771154"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["vector_file_name = 'VectorDB.json'\n","# vector_file_name = 'VectorDB_Mar_200_Fixed.json' # Marsillia embedding, fixed chunk size of (up to) 200 words, no overlap between chunks\n","# vector_file_name = 'VectorDB_Mar_100_Window.json' # Marsillia embedding, fixed chunk size of (up to) 100 words, with 10 words of overlap always\n","# vector_file_name = 'VectorDB_finbert.json'"],"metadata":{"id":"PeFvXiv8EPEn","executionInfo":{"status":"ok","timestamp":1755313831660,"user_tz":240,"elapsed":16,"user":{"displayName":"Nicholai Platonoff","userId":"15925676260455771154"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["EMBEDDING_MODEL = \"sujet-ai/Marsilia-Embeddings-EN-Large\" # this was the first model\n","# EMBEDDING_MODEL = \"ProsusAI/finbert\" # trained on sentiment classification, so it needed to append mean-pooling layer for sentence transformer\n","# EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # General purpose, fast\n","# EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"  # Higher quality"],"metadata":{"id":"SAMPj8vzEGFP","executionInfo":{"status":"ok","timestamp":1755313831666,"user_tz":240,"elapsed":9,"user":{"displayName":"Nicholai Platonoff","userId":"15925676260455771154"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["#### Queries and target"],"metadata":{"id":"Gz0ywqfiTbFY"}},{"cell_type":"code","source":["# 30 queries with target chunks.\n","queries_and_target = {\n","    \"From the January 2025 meeting, what is the Fed's current stance on the federal funds rate, and when might they cut?\": [\n","        \"All participants viewed it as appropriate to maintain the target range for the federal funds rate at 4-1/4 to 4-1/2 percent.\",\n","        \"Participants indicated that, provided the economy remained near maximum employment, they would want to see further progress on inflation before making additional adjustments to the target range for the federal funds rate.\",\n","    ],\n","\n","    \"From the January 2025 meeting, how does the Fed assess labor market conditions, such as unemployment and payroll gains?\": [\n","        \"Payroll gains had averaged 170,000 per month over the last three months of 2024 and the unemployment rate had stabilized at a relatively low level.\",\n","        \"Participants judged that labor market conditions had remained solid and that those conditions were broadly consistent with the Committee's goal of maximum employment\",\n","        \"The unemployment rate edged down to 4.1 percent in December,\",\n","    ],\n","\n","    \"From the January 2025 meeting, what is the Federal Reserve’s plan for its balance sheet in terms of its security holdings?\": [\n","        \"Participants judged that it was appropriate to continue the process of reducing the Federal Reserve's securities holdings.\",\n","        \"Roll over at auction the amount of principal payments from the Federal Reserve's holdings of Treasury securities maturing in each calendar month that exceeds a cap of $25 billion per month.\",\n","        \"Reinvest the amount of principal payments from the Federal Reserve's holdings of agency debt and agency mortgage‑backed securities (MBS) received in each calendar month that exceeds a cap of $35 billion per month into Treasury securities.\",\n","    ],\n","\n","    \"From the March 2025 meeting, how are tariffs and trade policy affecting the Federal Reserve’s outlook?\": [\n","        \"participants judged that inflation was likely to be boosted this year by the effects of higher tariffs, although significant uncertainty surrounded the magnitude and persistence of such effects.\",\n","        \"Several participants noted that the announced or planned tariff increases were larger and broader than many of their business contacts had expected.\",\n","        \"Several participants also noted that their contacts were already reporting increases in costs, possibly in anticipation of rising tariffs, or that their contacts had indicated willingness to pass on to consumers higher input costs that would arise from potential tariff increases.\",\n","    ],\n","\n","    \"From the March 2025 meeting, why did the Federal reserve slow the pace of balance sheet runoff and what does this signal?\": [\n","        \"Almost all participants supported a slowing of the pace of runoff at this meeting\",\n","        \"Roll over at auction the amount of principal payments from the Federal Reserve's holdings of Treasury securities maturing in March that exceeds a cap of $25 billion per month. Beginning on April 1, roll over at auction the amount of principal payments from the Federal Reserve's holdings of Treasury securities maturing in each calendar month that exceeds a cap of $5 billion per month.\",\n","        \"Some participants noted that a slower pace of runoff would also help guard against reserve scarcity emerging with little advance notice during a period of potentially rapid increase in the TGA.\",\n","        \"Most participants noted the importance of clearly communicating that slowing the runoff pace had no implications for the stance of monetary policy\",\n","    ],\n","\n","    \"From the March 2025 meeting, what are the Federal Reserve's main concerns about economic risks right now?\": [\n","        \"participants generally saw increased downside risks to employment and economic growth and upside risks to inflation while indicating that high uncertainty surrounded their economic outlooks.\",\n","        \"The staff judged that the risks around the baseline projections for economic activity and employment had tilted to the downside.\",\n","        \"Almost all participants pointed out that many market- or survey-based measures of near-term expected inflation had increased recently.\",\n","        \"Several participants emphasized that elevated inflation could prove to be more persistent than expected.\",\n","        \"Some participants observed, however, that the Committee may face difficult tradeoffs if inflation proved to be more persistent while the outlook for growth and employment weakened\",\n","    ],\n","\n","    \"From the May 2025 FOMC meeting, how severe were the tariff announcements and what was the market impact?\": [\n","        \"Participants assessed that the tariff increases announced so far had been significantly larger and broader than they had anticipated.\",\n","        \"The manager observed that market participants appeared to interpret recently announced trade policy changes as a negative supply shock that could restrain domestic activity relative to foreign activity in the near term.\",\n","        \"respondents to the Open Market Desk's Survey of Market Expectations had materially lowered their gross domestic product (GDP) forecasts and raised their inflation forecasts for this year while significantly increasing the probability they placed on a recession occurring within the next six months.\",\n","        \"Liquidity in foreign exchange markets deteriorated but was roughly in line with the historical relationship between liquidity and measures of market volatility.\",\n","    ],\n","\n","    \"From the May 2025 FOMC meeting, why did the dollar weaken despite higher U.S. rates, and what does this mean?\": [\n","        \"The dollar depreciated substantially against most major currencies, as the trade-weighted broad dollar index declined over 2 percent.\",\n","        \"The manager noted that the dollar had depreciated even though U.S. interest rates had risen more than foreign interest rates and prices of risky assets had declined, which historically have been associated with dollar appreciation.\",\n","        \"Market contacts attributed the decline to increased foreign exchange hedging prompted by heightened policy uncertainty and concerns that trade policy could pose greater downside risks to the U.S. than to other economies.\",\n","        \"Some participants commented on a change from the typical pattern of correlations across asset prices during the first half of April, with longer-term Treasury yields rising and the dollar depreciating despite the decline in the prices of equities and other risky assets. These participants noted that a durable shift in such correlations or a diminution of the perceived safe-haven status of U.S. assets could have long-lasting implications for the economy.\",\n","    ],\n","\n","    \"From the May 2025 meeting, how high is recession risk according to the Federal Reserve?\": [\n","        \"The staff viewed the possibility that the economy would enter a recession to be almost as likely as the baseline forecast\",\n","        \"The labor market was expected to weaken substantially, with the unemployment rate forecast moving above the staff's estimate of its natural rate by the end of this year and remaining above the natural rate through 2027\",\n","        \"Participants judged that downside risks to employment and economic activity and upside risks to inflation had risen, primarily reflecting the potential effects of tariff increases\",\n","        \"The staff continued to note the large amount of uncertainty surrounding trade policy and other economic policies and now viewed the uncertainty around the projection as elevated relative to the average over the past 20 years.\",\n","    ],\n","\n","    \"From the June 2025 FOMC meeting, has the Federal Reserve more seriously considered rate cuts and are they coming soon?\": [\n","        \"Most participants assessed that some reduction in the target range for the federal funds rate this year would likely be appropriate\",\n","        \"A couple of participants noted that, if the data evolve in line with their expectations, they would be open to considering a reduction in the target range for the policy rate as soon as at the next meeting\",\n","        \"The median respondent's modal path for the federal funds rate in the June survey shifted higher through 2026 and implied two 25 basis point rate cuts both this year and next year\",\n","    ],\n","\n","    \"From the June 2025 FOMC meeting, what happened with the trade tensions and tariffs since May?\": [\n","        \"Markets were attentive to the de-escalation of trade tensions\",\n","        \"In mid-May, the U.S. and China agreed to a 90-day reduction in bilateral tariffs, and recent indicators suggested that this change led to a rebound in trade flows\",\n","        \"Participants observed that uncertainty about the economic outlook had diminished amid a reduction in announced and expected tariffs, which appeared to peak in April and had subsequently declined\",\n","        \"The median respondent's expectations for real gross domestic product (GDP) growth and personal consumption expenditures (PCE) inflation for 2025 retraced some of the moves that occurred after the April tariff announcements\",\n","    ],\n","\n","    \"From the June 2025 FOMC meeting, how much has inflation improved and what's the outlook?\": [\n","        \"Total consumer price inflation—as measured by the 12-month change in the PCE price index—was estimated to have been 2.3 percent in May, based on the consumer and producer price indexes\",\n","        \"Core PCE price inflation, which excludes changes in consumer energy prices and many consumer food prices, was 2.6 percent in May\",\n","        \"Both total and core inflation were lower than at the beginning of the year\",\n","        \"Some participants observed that services price inflation had moved down recently, while goods price inflation had risen\",\n","        \"Participants noted that longer-term inflation expectations continued to be well anchored\",\n","    ],\n","\n","\n","    \"What are Amazon’s total capitalized costs for video and music for the years 2023 and 2024?\": [\n","        \"The total capitalized costs of video, which is primarily released content, and music as of December 31, 2023 and 2024 were $ 17.4 billion and $ 19.6 billion.\",\n","    ],\n","\n","    \"How many shareholders did Apple report having in 2024?\": [\n","        \"As of October 18, 2024, there were 23,301 shareholders of record.\",\n","        \"Prompt: How did the net sales of Apple’s products and services change from 2023 to 2024?\",\n","\n","    ],\n","\n","    \"How did the net sales of Apple’s products and services change from 2023 to 2024?\": [\n","        \"iPhone net sales were relatively flat during 2024 compared to 2023.\",\n","        \"Mac net sales increased during 2024 compared to 2023 due primarily to higher net sales of laptops.\",\n","        \"iPad net sales decreased during 2024 compared to 2023 due primarily to lower net sales of iPad Pro and the entry-level iPad models, partially offset by higher net sales of iPad Air.\",\n","        \"Wearables, Home and Accessories net sales decreased during 2024 compared to 2023 due primarily to lower net sales of Wearables and Accessories.\",\n","        \"Services net sales increased during 2024 compared to 2023 due primarily to higher net sales from advertising, the App Store ® and cloud services.\",\n","    ],\n","\n","    \"Describe the change in net sales for the iPhone, Mac, and iPad from 2023 to 2024.\": [\n","        \"iPhone net sales were relatively flat during 2024 compared to 2023.\",\n","        \"Mac net sales increased during 2024 compared to 2023 due primarily to higher net sales of laptops.\",\n","        \"iPad net sales decreased during 2024 compared to 2023 due primarily to lower net sales of iPad Pro and the entry-level iPad models, partially offset by higher net sales of iPad Air.\",\n","        \"Prompt: What were the amounts of dividends paid to stockholders of Alphabet Inc.’s Class A, Class B, and Class C shares for the year of 2024?\",\n","        \"Dividend payments to stockholders of Class A, Class B, and Class C shares were $3.5 billion, $519 million, and $3.3 billion , respectively, for the year ended December 31, 2024.\",\n","        \"Prompt: How did Alphabet Inc.’s revenue change from 2023 to 2024? What was the change driven by?\",\n","        \"Revenues were $350.0 billion, an increase of 14% year over year, primarily driven by an increase in Google Services revenues of $32.4 billion, or 12%, and an increase in Google Cloud revenues of $10.1 billion, or 31%.\",\n","    ],\n","\n","    \"How did Meta’s total revenue change from 2023 to 2024? What was it driven by?\": [\n","        \"Total revenue for 2024\",\n","        \"was $164.50 billion, an increase of 22% compared to 2023, due to an increase in advertising revenue.\",\n","    ],\n","\n","    \"How much in dividends did Meta pay in 2024?\": [\n","        \"Beginning in February 2024, our board of directors declared a quarterly cash dividend of $0.50 per share to the holders of our Class A and Class B common stock.\",\n","        \"RSUs granted on or after March 1, 2024 under our 2012 Equity Incentive Plan (Amended 2012 Plan), which was most recently amended in May 2024, are entitled to dividend equivalent rights.\",\n","        \"During the year ended December 31, 2024, total dividend and dividend equivalent payments were $4.38 billion and $691 million for Class A and Class B common stock, respectively.\",\n","    ],\n","\n","    \"What are the highlights of Microsoft’s performance from fiscal year 2025 compared with fiscal year 2024?\": [\n","        \"Fiscal Year 2025 Compared with Fiscal Year 2024\",\n","        \"Revenue increased $36.6 billion or 15% with growth across each of our segments.\",\n","    ],\n","\n","    \"How much in dividends did Microsoft pay to shareholders in 2025 and 2024?\": [\n","        \"During fiscal years 2025 and 2024, our Board of Directors declared dividends totaling $24.7 billion\",\n","        \"and $22.3 billion, respectively.\",\n","        \"Prompt: How much has Microsoft spent on advertising in the past few years?\",\n","        \"Advertising expense was $ 2.1 billion, $ 1.7 billion, and $ 904 million in fiscal years 2025, 2024, and 2023, respectively.\",\n","    ],\n","\n","    \"How did the Fed's stance on interest rate cuts evolve throughout 2025?\": [\n","        \"Participants indicated that, provided the economy remained near maximum employment, they would want to see further progress on inflation before making additional adjustments to the target range for the federal funds rate\",\n","        \"Participants noted that policy decisions were not on a preset course and were conditional on the evolution of the economy\",\n","        \"With economic growth and the labor market still solid and current monetary policy restrictive, participants assessed that the Committee was well positioned to wait for more clarity on the outlook for inflation and economic activity\",\n","        \"In discussing the outlook for monetary policy, participants remarked that uncertainty about the net effect of an array of government policies on the economic outlook was high, making it appropriate to take a cautious approach\",\n","        \"In considering the outlook for monetary policy, participants agreed that with economic growth and the labor market still solid and current monetary policy moderately restrictive, the Committee was well positioned to wait for more clarity on the outlooks for inflation and economic activity\",\n","        \"Participants agreed that uncertainty about the economic outlook had increased further, making it appropriate to take a cautious approach\",\n","        \"Most participants assessed that some reduction in the target range for the federal funds rate this year would likely be appropriate\",\n","        \"A couple of participants noted that, if the data evolve in line with their expectations, they would be open to considering a reduction in the target range for the policy rate as soon as at the next meeting\",\n","    ],\n","\n","    \"What was Google's total revenue in Q2 2025?\": [\n","        \"Consolidated revenue of $96.4 billion increased by 14% or 13% in constant currency.\",\n","        \"Consolidated Revenue: $96.4 billion, up 14%, reflecting double-digit gains in Search, YouTube, subscriptions, and Cloud.\",\n","    ],\n","\n","    \"How many GitHub Copilot users does Microsoft have?\": [\n","        \"All-up, we now have over 15 million GitHub Copilot users, up over 4X year-over-year.\",\n","        \"And both digital natives like Twilio and enterprises like Cisco, HPE, SkyScanner, and Target, continue to choose GitHub Copilot to equip their developers with AI throughout the entire dev lifecycle.\",\n","    ],\n","\n","    \"What is Amazon's current AWS revenue run rate?\": [\n","        \"AWS now has an annualized revenue run rate of more than $117 billion.\",\n","        \"Moving next to our AWS segment, revenue was $29.3 billion in Q1, an increase of 17% year-over-year. AWS now has an annualized revenue run rate of more than $117 billion.\",\n","        \"AWS grew 17% year-over-year in Q1 and now sits at a $117 billion annualized revenue run rate.\",\n","    ],\n","\n","    \"Compare the AI infrastructure investments across Google, Meta, and Amazon in 2025\": [\n","        \"Google: Moving to investments, given the strong demand for our cloud products and services, we now expect to invest approximately $85 billion in CapEx in 2025, up from a previous estimate of $75 billion.\",\n","        \"Amazon: Now, turning to our cash CapEx, which was $24.3 billion in Q1. The majority of the spend is to support the growing need for technology infrastructure. This primarily relates to AWS as we invest to support demand for our AI services, and increasingly in custom silicon like Trainium.\",\n","        \"Meta: We anticipate our full year 2025 capital expenditures, including principal payments on finance leases, will be in the range of $64-72 billion, increased from our prior outlook of $60-65 billion.\",\n","    ],\n","\n","    \"What are the main growth drivers for cloud services mentioned by AWS and Google Cloud?\": [\n","        \"Google Cloud: Google Cloud backlog increased 18% sequentially in Q2 and 38% year over year, reaching $106 billion at the end of the quarter. This growth was driven by strong demand for our products and services from both new and existing customers.\",\n","        \"Google Cloud: One, the number of deals over $250 million doubling year over year. Two, in the first half of 2025, we signed the same number of deals over $1 million that we did in all of 2024. Three, the number of new GCP customers increased by nearly 28% quarter over quarter.\",\n","        \"AWS: During the first quarter, we continue to see growth in both Generative AI business and non-Generative AI offerings. As companies turn their attention to newer initiatives, bring more workloads to the cloud, restart or accelerate existing migrations from on-premises to the cloud, and tap into the power of Generative AI.\",\n","    ],\n","\n","    \"How are companies addressing AI capacity constraints and what solutions are they implementing?\": [\n","        \"Google: We have been working hard to increase capacity and have improved the pace of server deployment. We expect to remain in a tight demand-supply environment going into 2026.\",\n","        \"Amazon: At the bottom layer for those building models, our new custom AI chip Trainium 2 is starting to lay in capacity in larger quantities with significant appeal and demand... So, the 30% to 40% better price performance that Trainium 2 offers versus other GPU based instances is compelling.\",\n","        \"Microsoft: In our AI services, while we continue to bring datacenter capacity online as planned, demand is growing a bit faster. Therefore, we now expect to have some AI capacity constraints beyond June.\",\n","        \"Meta: So even with the capacity that we're bringing online in 2025, we are having a hard time meeting the demand that teams have for compute resources across the company.\",\n","    ],\n","\n","    \"What are the different approaches to AI model efficiency and cost reduction across these tech companies?\": [\n","        \"Amazon: For AI to be as successful as we believe it can be, the price of inference needs to come down significantly. We consider this part of our mission and responsibility to help make it so.\",\n","        \"Google: As we ramp our AI investments, we continue to focus on driving improvements in productivity and efficiency to offset growth in technical infrastructure-related expenses, particularly from higher depreciation.\",\n","        \"Meta: A big focus of this work will be on developing increasingly efficient recommendation systems so that we can continue scaling up the complexity and compute used to train our models while avoiding diminishing returns.\",\n","        \"Microsoft: We continue to optimize and drive efficiencies across every layer – from DC design, to hardware and silicon, to systems software, to model optimization – all towards lowering costs and increasing performance.\",\n","    ],\n","\n","    \"How are macroeconomic uncertainties affecting capital expenditure decisions in Q2 2025?\": [\n","        \"Meta: Before moving to our financial guidance, I want to acknowledge the dynamic macro environment and note that our range reflects the potential for a wider set of outcomes.\",\n","        \"Google: However, volatility in exchange rates could affect the impact of FX on Q3 revenue.\",\n","        \"Amazon: The external environment remains complex, and as we have done throughout our history, we are focused on the inputs that we can control to protect the customer experience.\",\n","        \"Meta: The higher costs we expect to incur for infrastructure hardware this year really comes from suppliers who source from countries around the world. And there's just a lot of uncertainty around this, given the ongoing trade discussions.\",\n","    ],\n","\n","    \"Compare the monetization strategies for AI services across Google, Microsoft, Meta, and Amazon\": [\n","        \"Google: Advertisers that activate AI Max in search campaigns typically see 14% more conversions, and Campaigns using Smart Bidding Exploration see a 19% increase in conversions on average.\",\n","        \"Google: Management introduced the AgentSpace enterprise agent platform and reported over one million subscriptions booked ahead of general availability.\",\n","        \"Meta: We are also starting to introduce ads on unmonetized surfaces, like Threads, which we opened up to all eligible advertisers this month to reach people in over 30 different markets to start, including the US.\",\n","        \"Meta: In Q1, we introduced our new Generative Ads Recommendation model, or GEM, for ads ranking. This model uses a new architecture we developed that is twice as efficient at improving ad performance.\",\n","    ],\n","\n","    \"What are the exact metrics Meta provided about Threads usage and monetization plans?\": [\n","        \"We are also starting to introduce ads on unmonetized surfaces, like Threads, which we opened up to all eligible advertisers this month to reach people in over 30 different markets to start, including the US.\",\n","        \"We don't expect Threads to be a meaningful driver of overall impression or revenue growth in 2025.\",\n","        \"We began testing using Llama in Threads recommendation systems at the end of last year given the app's text-based content, and have already seen a 4% lift in time spent from the first launch.\",\n","        \"As we do for any newly monetized surface, we expect to gradually ramp ad supply as we optimize the ad formats and ensure they feel native to the app.\",\n","    ]\n","}"],"metadata":{"id":"h9ubdRnNF2jB","executionInfo":{"status":"ok","timestamp":1755313831769,"user_tz":240,"elapsed":104,"user":{"displayName":"Nicholai Platonoff","userId":"15925676260455771154"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#### Loading the datasets"],"metadata":{"id":"Fw7OIMvfE1EJ"}},{"cell_type":"markdown","source":["**Loading JSON Vector DB:**"],"metadata":{"id":"FRkDSQmv73I2"}},{"cell_type":"code","source":["path = f'{file_path}/{vector_file_name}'\n","\n","with open(path, \"r\") as file:\n","    Vector_db = json.load(file)"],"metadata":{"id":"53bl88I171vC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sample of how to access\n","Vector_db['FOMC_min_JAN25.txt'][0]['chunk_text']"],"metadata":{"id":"FFc8kTbY9i96"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Embedding Model"],"metadata":{"id":"pExeoFLwFNV4"}},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","\n","model = SentenceTransformer(EMBEDDING_MODEL)"],"metadata":{"id":"LdeF60GBFP-w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Version that handles duplicate chunks:**"],"metadata":{"id":"p46Wa_vOUs72"}},{"cell_type":"code","source":["flat_chunks = []\n","all_embeddings = []\n","chunk_metadata = []\n","seen_chunks = {}  # set of chunk texts\n","\n","for doc_name, chunks in Vector_db.items():\n","    for chunk in chunks:\n","      chunk_text = chunk['chunk_text']\n","\n","      # Check if we've seen exact text before\n","      if chunk_text not in seen_chunks:\n","          # First occurrence: add to set\n","          seen_chunks[chunk_text] = len(flat_chunks)\n","\n","          # normal processing\n","          flat_chunks.append(chunk_text)\n","          all_embeddings.append(chunk['embed'])\n","          chunk_metadata.append({\n","              'source_document': doc_name,\n","              'chunk_id': chunk['chunk_id'],\n","              'original_index': len(flat_chunks) - 1\n","          })\n","      else:\n","          # Duplicate, comment out to avoid log\n","          print(f\"Skipping duplicate from {doc_name}, chunk_id: {chunk['chunk_id']}\")\n","          # continue  # need this statment if cooment out log"],"metadata":{"collapsed":true,"id":"xRyzHOLFVIag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(flat_chunks), len(all_embeddings), len(chunk_metadata) # 2000 less chunks than before (for sentence VectorDB)"],"metadata":{"id":"YKfQtoA1X6Qi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# instead of cosine similarity we are taking the dot product\n","# this code is part of sentence chunking and create json\n","import faiss\n","import numpy as np\n","\n","vecs = np.asarray(all_embeddings, dtype=np.float32)\n","print(f\"Loaded {len(flat_chunks)} chunks with embeddings of shape: {vecs.shape}\")\n","\n","index = faiss.IndexFlatIP(vecs.shape[1])\n","index.add(vecs)         # index used in retreive function\n","print(f\"Indexed {index.ntotal} chunks of text.\")"],"metadata":{"id":"B7orVfMAFRuC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Language Model (LLAMA)"],"metadata":{"id":"MTjk7DwVFayJ"}},{"cell_type":"code","source":["# API call to the model\n","from huggingface_hub import InferenceClient\n","\n","client = InferenceClient(model=\"meta-llama/Llama-3.1-8B-Instruct\")"],"metadata":{"id":"iWNnKPlpFnLF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Baseline (No RAG)**"],"metadata":{"id":"xcXGh0hFIKDq"}},{"cell_type":"code","source":["def baseline(input_query, client):\n","    instruction_prompt = '''You are a helpful chatbot. Answer using your general knowledge.'''\n","\n","    try:\n","        messages = [\n","            {'role': 'system', 'content': instruction_prompt},\n","            {'role': 'user', 'content': input_query},\n","        ]\n","\n","        response = client.chat_completion(messages=messages, max_tokens=512)\n","\n","        print('Chatbot response:')\n","        print(response.choices[0].message.content)\n","\n","    except Exception as e:\n","        print(f\"Error: {e}\")"],"metadata":{"id":"myXWSAwVFnrY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**RAG implementation start:**"],"metadata":{"id":"wiXIVEyqIN36"}},{"cell_type":"code","source":["def retrieve(query, top_k):\n","    query_vector = model.encode([query], normalize_embeddings=True)\n","    query_vector = query_vector.astype(np.float32)\n","\n","    scores, indices = index.search(query_vector, top_k)\n","\n","    results = []\n","    for i in range(len(indices[0])):\n","        idx = indices[0][i]\n","        score = scores[0][i]\n","\n","        if idx < len(flat_chunks):\n","            results.append({\n","                \"text\": flat_chunks[idx].strip(),\n","                \"score\": float(score),\n","                \"source\": f\"{chunk_metadata[idx]['source_document']}_chunk_{chunk_metadata[idx]['chunk_id']}\"\n","            })\n","\n","    return results"],"metadata":{"id":"Vp28x2rTU6Ju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to extract the source used in the chatbot response\n","import re\n","\n","def extract_source(text):\n","    \"\"\"Extract all text within square brackets using regex\"\"\"\n","    pattern = r'\\[([^\\]]+)\\]'\n","    matches = re.findall(pattern, text)\n","    return matches"],"metadata":{"id":"45KkW2U6TSRL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def containment_score(retrieved_chunk, target_excerpt):\n","    chunk_tokens = set(retrieved_chunk.lower().split())\n","    target_tokens = set(target_excerpt.lower().split())\n","\n","    intersection = len(chunk_tokens & target_tokens)\n","    target_size = len(target_tokens)\n","\n","    # What fraction of the target is contained in the chunk?\n","    return intersection / target_size if target_size > 0 else 0\n","\n","def is_match(retrieved_chunk, target_excerpt, threshold=0.6): # 0.6 is somewhat generous\n","    # Check if target is substantially contained in chunk\n","    if containment_score(retrieved_chunk, target_excerpt) >= threshold:\n","        return True\n","\n","    # Also check if target appears as exact substring\n","    if target_excerpt.lower() in retrieved_chunk.lower():\n","        return True\n","\n","    return False\n","\n","def calculate_precision_recall_retrieve(target_passages, k_retrieved_chunks):\n","  '''\n","  Calculates precison and recall of a single pair of target passages\n","  and its corresponding k_retrieved passages that come from the retrive()\n","  function.\n","\n","  Uses containment_score() and is_match() functions. to determine if the retrieved\n","  chunk has enough similar words as the target passage\n","\n","  '''\n","  match_cnt = 0\n","  for tgt in target_passages:\n","    for rtr in k_retrieved_chunks:\n","      if is_match(rtr, tgt): # above function\n","        match_cnt+=1\n","        # print(\"Match:\")\n","        # print(\"Retrieved:\",rtr)\n","        # print(\"Target:\", tgt, \"\\n\")\n","  recall = match_cnt/len(target_passages)\n","  precision = match_cnt/len(k_retrieved_chunks)\n","  return [precision, recall]"],"metadata":{"id":"cUZiWyV0X98v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rag_implementation(input_query, client, target_passages = None, top_k = 100):\n","    retrieved_knowledge = retrieve(input_query, top_k) # here we can manipulate top_k values\n","\n","    '''print('Retrieved knowledge:')\n","    for idx, hit in enumerate(retrieved_knowledge, 1):\n","        print(f' -(similarity: {hit[\"score\"]:.2f}) [{hit[\"source\"]}] {hit[\"text\"]}')'''\n","\n","    # build the system prompt with source tags\n","    context_lines = [f' - [{h[\"source\"]}] {h[\"text\"]}' for h in retrieved_knowledge]\n","    retrieved_docs = [f'{h[\"source\"]}' for h in retrieved_knowledge]\n","    retrieved_text = [f'{h[\"text\"]}' for h in retrieved_knowledge]\n","\n","    # immediatly calculate the precision/recall for this query:\n","    if target_passages is not None:\n","        precision_recall_pair = calculate_precision_recall_retrieve(target_passages, k_retrieved_chunks = retrieved_text)\n","        print(\"Precision:\", precision_recall_pair[0])\n","        print(\"Recall:\", precision_recall_pair[1])\n","    else:\n","        precision_recall_pair = None\n","\n","\n","    instruction_prompt = (\n","        \"You are a helpful chatbot.\\n\"\n","        \"Use only the following pieces of context to answer the question. \"\n","        \"Cite the source ID in square brackets after each fact:\\n\"\n","        + \"\\n\".join(context_lines)\n","    )\n","\n","    try:\n","        messages = [\n","                {'role': 'system', 'content': instruction_prompt},\n","                {'role': 'user', 'content': input_query},\n","            ]\n","\n","        response = client.chat_completion(messages=messages, max_tokens=512)\n","        response_text = response.choices[0].message.content\n","        relevant_docs = extract_source(response_text)\n","\n","        print('\\nChatbot response:')\n","        print(response_text)\n","\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n","\n","    # response_text = \"Dummy Chatbot Response Text, (Avoiding Usage Limit)\"\n","    # relevant_docs = [\"DUMMY:    FOMC_min_JAN25.txt_chunk_17\"]\n","\n","    return retrieved_docs, relevant_docs, retrieved_text, precision_recall_pair, response_text"],"metadata":{"id":"bZytseK-YWBl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Manually Query the ChatBot:**"],"metadata":{"id":"I6UEaQH9hPwI"}},{"cell_type":"code","source":["input_query = input('\\nAsk me a question: ')\n","\n","print('-'*50)\n","print('BASELINE:')\n","baseline(input_query, client)\n","\n","print('-'*50)\n","print('RAG IMPLEMENTATION:')\n","_, _, _, _, _ = rag_implementation(input_query, client)"],"metadata":{"id":"7XG8venLTz2U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Precision and Recall"],"metadata":{"id":"75vIBPmsPtWg"}},{"cell_type":"markdown","source":["**Target json file:** Create each time you run an experiment."],"metadata":{"id":"ArbC9zmsTfIB"}},{"cell_type":"code","source":["import json\n","import os\n","\n","# delete if exists\n","path_log = f'{file_path}/target_sample.json'\n","if os.path.exists(path_log):\n","  os.remove(path_log)"],"metadata":{"id":"6KVp5TF7Ukey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_initial_json(filename=\"data.json\"):\n","    \"\"\"Create initial JSON file with empty lists\"\"\"\n","    initial_data = {\n","        \"queries\": [],\n","        \"retrieved_docs\": [],\n","        \"relevant_docs\": [],\n","        \"reference_answers\": [],\n","        \"retrieved_answers\": [],\n","        \"precision_recall_pair\": [],  #new\n","        \"generated_answers\": []\n","    }\n","\n","    with open(filename, 'w', encoding='utf-8') as f:\n","        json.dump(initial_data, f, indent=2)\n","    print(f\"Created {filename}\")\n","\n","file_name = f'{file_path}/target_sample.json'\n","create_initial_json(file_name)"],"metadata":{"id":"rG7Ved50ZMmX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# not changed\n","def append_to_json(filename, key, value):\n","    \"\"\"Append value to specified key in JSON file\"\"\"\n","    # Load existing data\n","    with open(filename, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    # Append new value\n","    if key in data and isinstance(data[key], list):\n","        data[key].append(value)\n","\n","    # Save updated data\n","    with open(filename, 'w', encoding='utf-8') as f:\n","        json.dump(data, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"Added '{value}' to '{key}'\")"],"metadata":{"id":"5lrb4pbATr9C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TOP_K = 100 # for experiments\n","for query, answers in queries_and_target.items():\n","    # now pass 'answers' variable to rag_implementation() to calculate precision_recall for retrival\n","    retrieved_docs, relevant_docs, retrieved_ans, precision_recall_pair, response_text = rag_implementation(query, client, answers, TOP_K)\n","    append_to_json(file_name, \"queries\", query)                          # the actual query\n","    append_to_json(file_name, \"retrieved_docs\", retrieved_docs)          # the top-k retrieved chunks IDs as list of string, exp \"FOMC_min_JAN25.txt_chunk_57\"\n","    append_to_json(file_name, \"relevant_docs\", relevant_docs)            # the chunks IDs that were actually used and cited in the response., also like \"FOMC_min_JAN25.txt_chunk_57\"\n","    append_to_json(file_name, \"reference_answers\", answers)              # the actual text target_passages corresponding to the query\n","    append_to_json(file_name, \"retrieved_answers\", retrieved_ans)        # the top-k chunks as a list of strings (now the actual chunk text)\n","    append_to_json(file_name, \"precision_recall_pair\", precision_recall_pair) # new\n","    append_to_json(file_name, \"generated_answers\", response_text)        # chat-bot response"],"metadata":{"id":"a07OuvggZ3Xe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Precision and Recall**"],"metadata":{"id":"bjvIbWBgTngD"}},{"cell_type":"code","source":["def calculate_precision_recall_output(queries, retrieved_docs, relevant_docs):\n","    # Calculate precision and recall for a list of queries.\n","    total_precision = 0\n","    total_recall = 0\n","    num_queries = len(queries)\n","\n","    for i in range(num_queries):\n","        retrieved = set(retrieved_docs[i])\n","        relevant = set(relevant_docs[i])\n","\n","        if not retrieved:\n","            precision = 0\n","        else:\n","            precision = len(retrieved.intersection(relevant)) / len(retrieved)\n","\n","        if not relevant:\n","            recall = 0\n","        else:\n","            recall = len(retrieved.intersection(relevant)) / len(relevant)\n","\n","        total_precision += precision\n","        total_recall += recall\n","\n","    average_precision = total_precision / num_queries\n","    average_recall = total_recall / num_queries\n","\n","    return {\"average_precision\": average_precision, \"average_recall\": average_recall}"],"metadata":{"id":"C-w6XQnLPyKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","with open(file_name, 'r') as f:\n","    data = json.load(f)\n","\n","\n","queries = data[\"queries\"]\n","retrieved_docs = data[\"retrieved_docs\"]\n","relevant_docs = data[\"relevant_docs\"]\n","retrieved_chunks = data[\"retrieved_answers\"]\n","reference_answers = data[\"reference_answers\"]\n","precision_recall_pair = data[\"precision_recall_pair\"]\n","generated_response = data[\"generated_answers\"]"],"metadata":{"id":"f1ZTxNpXTVUb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####**Precision Recall For Retrival:**"],"metadata":{"id":"kYDtitxNi3Nu"}},{"cell_type":"code","source":["mean_prec_retr = np.mean(np.array(precision_recall_pair)[:, 0])\n","mean_recall_retr = np.mean(np.array(precision_recall_pair)[:, 1])\n","print(\"VectorDB used:\", vector_file_name)\n","print(\"Top-K value used for experiment:\", TOP_K)\n","print(\"Mean Precision for Retrival:\", round(mean_prec_retr, 4))\n","print(\"Mean Recall for Retrival:\", round(mean_recall_retr,4))"],"metadata":{"id":"yxKxVxy2fTIQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####**Precision Recall For Output:**"],"metadata":{"id":"kYXzZtvejDDb"}},{"cell_type":"code","source":["metrics = calculate_precision_recall_output(queries, retrieved_docs, relevant_docs)\n","print(metrics) # from old run"],"metadata":{"id":"mQ2dMQGSTjFt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### ROUGE and BLEU score"],"metadata":{"id":"HhLZoQS92AH3"}},{"cell_type":"code","source":["from rouge_score import rouge_scorer\n","import statistics\n","\n","def calculate_rouge(text1, text2):\n","    \"\"\"\n","    Calculate ROUGE scores between two texts.\n","    Returns a dictionary with rouge1, rouge2, and rougeL scores.\n","    \"\"\"\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    scores = scorer.score(text1, text2)\n","\n","    return {\n","        'rouge1': scores['rouge1'].fmeasure,\n","        'rouge2': scores['rouge2'].fmeasure,\n","        'rougeL': scores['rougeL'].fmeasure\n","    }"],"metadata":{"id":"m0ix3LdPJNW-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","def calculate_bleu(text1, text2):\n","    \"\"\"\n","    Calculate BLEU score between two texts.\n","    \"\"\"\n","    reference = [text1.split()]\n","    candidate = text2.split()\n","    bleu_score = sentence_bleu(reference, candidate)\n","\n","    return bleu_score"],"metadata":{"id":"xeNzKJaqJXmU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Between generated and reference texts**\n","This calculates the similarity between the final answer generated by the language model (after considering the retrieved chunks) and your manually created reference answers. This measures the quality of the generated response and how well it captures the information in the reference answers. It tells you how good your system's output is."],"metadata":{"id":"ImEZiNULEU6v"}},{"cell_type":"code","source":["# Download NLTK data if not already downloaded\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError: # Changed from nltk.downloader.DownloadError to LookupError\n","    nltk.download('punkt')\n","\n","rouge_scores_generated_vs_reference = []\n","bleu_scores_generated_vs_reference = []\n","\n","for i in range(len(generated_response)):\n","    generated = generated_response[i]\n","    references = reference_answers[i]\n","\n","    rouge_scores_per_query = []\n","    bleu_scores_per_query = []\n","\n","    for reference in references:\n","        # Calculate ROUGE scores\n","        rouge_score = calculate_rouge(reference, generated)\n","        rouge_scores_per_query.append(rouge_score)\n","\n","        # Calculate BLEU scores\n","        bleu_score = calculate_bleu(reference, generated)\n","        bleu_scores_per_query.append(bleu_score)\n","\n","    # Average ROUGE scores for the current query\n","    avg_rouge_rouge1 = statistics.mean([score['rouge1'] for score in rouge_scores_per_query])\n","    avg_rouge_rouge2 = statistics.mean([score['rouge2'] for score in rouge_scores_per_query])\n","    avg_rouge_rougel = statistics.mean([score['rougeL'] for score in rouge_scores_per_query])\n","\n","    rouge_scores_generated_vs_reference.append({\n","        'rouge1': avg_rouge_rouge1,\n","        'rouge2': avg_rouge_rouge2,\n","        'rougeL': avg_rouge_rougel\n","    })\n","\n","    # Average BLEU scores for the current query\n","    avg_bleu_score = statistics.mean(bleu_scores_per_query)\n","    bleu_scores_generated_vs_reference.append(avg_bleu_score)\n","\n","# Calculate overall average ROUGE and BLEU scores across all queries\n","overall_avg_rouge1 = statistics.mean([score['rouge1'] for score in rouge_scores_generated_vs_reference])\n","overall_avg_rouge2 = statistics.mean([score['rouge2'] for score in rouge_scores_generated_vs_reference])\n","overall_avg_rougel = statistics.mean([score['rougeL'] for score in rouge_scores_generated_vs_reference])\n","overall_avg_bleu = statistics.mean(bleu_scores_generated_vs_reference)\n","\n","print(\"Average ROUGE scores (Generated vs Reference):\")\n","print(f\"  ROUGE-1: {overall_avg_rouge1:.4f}\")\n","print(f\"  ROUGE-2: {overall_avg_rouge2:.4f}\")\n","print(f\"  ROUGE-L: {overall_avg_rougel:.4f}\")\n","print(\"\\nAverage BLEU score (Generated vs Reference):\")\n","print(f\"  BLEU: {overall_avg_bleu:.4f}\")"],"metadata":{"id":"Jp6_gSCKJzFI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Between retrieved chunks and reference texts**\n","This calculates the similarity between the individual chunks of text that were retrieved from your knowledge base and your reference answers. This measures the quality of the retrieval process and how well the system is finding relevant information. It tells you if the RAG system is providing the language model with useful context."],"metadata":{"id":"QhoN_fFOEaIT"}},{"cell_type":"code","source":["rouge_scores_retrieved_vs_reference = []\n","bleu_scores_retrieved_vs_reference = []\n","\n","for i in range(len(retrieved_chunks)):\n","    retrieved_chunk_list = retrieved_chunks[i]\n","    references = reference_answers[i]\n","\n","    rouge_scores_per_query = []\n","    bleu_scores_per_query = []\n","\n","    for retrieved_chunk in retrieved_chunk_list: # Iterate through individual retrieved chunks\n","        for reference in references:\n","            # Calculate ROUGE scores\n","            rouge_score = calculate_rouge(reference, retrieved_chunk)\n","            rouge_scores_per_query.append(rouge_score)\n","\n","            # Calculate BLEU scores\n","            bleu_score = calculate_bleu(reference, retrieved_chunk)\n","            bleu_scores_per_query.append(bleu_score)\n","\n","    # Average ROUGE scores for the current query\n","    avg_rouge_rouge1 = statistics.mean([score['rouge1'] for score in rouge_scores_per_query])\n","    avg_rouge_rouge2 = statistics.mean([score['rouge2'] for score in rouge_scores_per_query])\n","    avg_rouge_rougel = statistics.mean([score['rougeL'] for score in rouge_scores_per_query])\n","\n","    rouge_scores_retrieved_vs_reference.append({\n","        'rouge1': avg_rouge_rouge1,\n","        'rouge2': avg_rouge_rouge2,\n","        'rougeL': avg_rouge_rougel\n","    })\n","\n","    # Average BLEU scores for the current query\n","    avg_bleu_score = statistics.mean(bleu_scores_per_query)\n","    bleu_scores_retrieved_vs_reference.append(avg_bleu_score)\n","\n","\n","# Calculate overall average ROUGE and BLEU scores across all queries\n","overall_avg_rouge1 = statistics.mean([score['rouge1'] for score in rouge_scores_retrieved_vs_reference])\n","overall_avg_rouge2 = statistics.mean([score['rouge2'] for score in rouge_scores_retrieved_vs_reference])\n","overall_avg_rougel = statistics.mean([score['rougeL'] for score in rouge_scores_retrieved_vs_reference])\n","overall_avg_bleu = statistics.mean(bleu_scores_retrieved_vs_reference)\n","\n","print(\"Average ROUGE scores (Retrieved vs Reference):\")\n","print(f\"  ROUGE-1: {overall_avg_rouge1:.4f}\")\n","print(f\"  ROUGE-2: {overall_avg_rouge2:.4f}\")\n","print(f\"  ROUGE-L: {overall_avg_rougel:.4f}\")\n","print(\"\\nAverage BLEU score (Retrieved vs Reference):\")\n","print(f\"  BLEU: {overall_avg_bleu:.4f}\")"],"metadata":{"id":"fUiEDioqEdy1"},"execution_count":null,"outputs":[]}]}